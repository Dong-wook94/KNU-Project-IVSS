{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRGAN model\n",
    "<img src=\"model.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PReLU(_x):\n",
    "    alpha = tf.get_variable(\"alpha\", _x.get_shape()[-1], \n",
    "                            initializer = tf.constant_initializer(0.0), dtype = tf.int32)\n",
    "    return tf.nn.relu(_x) + alpha * (_x - tf.abs(_x)) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pixel shuffle\n",
    "\n",
    "\\\\(S_s  : [0,1]\\\\)<sup>(sH * sW * C)</sup> -> \\\\([0,1]\\\\)<sup>(H * W * s<sup>2</sup>c)</sup>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\\\\(S_s(I)\\\\)<sub>i,j,k</sub> = \\\\(I\\\\)<sub>si+k%s, sj+(k%s), k/s<sup>2</sup></sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phaseShift(inputs, scale, shape_1, shape_2):\n",
    "    X = tf.reshape(inputs, shape_1)\n",
    "    X = tf.transpose(X, [0, 1, 3, 2, 4])\n",
    "    return tf.reshape(X, shape_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixelShuffler(inputs, scale = 2):\n",
    "    size = tf.shape(inputs)\n",
    "    batch_size = size[0]\n",
    "    h = size[1]\n",
    "    w = size[2]\n",
    "    c = inputs.get_shape().as_list()[-1]\n",
    "    \n",
    "    # Get the target channel size\n",
    "    channel_target = c // (scale**2)\n",
    "    channel_factor = c // channel_target\n",
    "    \n",
    "    shape_1 = [batch_size, h, w, channel_factor // scale, channel_factor // scale]\n",
    "    shape_2 = [batch_size, h * scale, w * scale, 1]\n",
    "    \n",
    "    # Reshape and transpose for periodic shuffling for each channel\n",
    "    \n",
    "    input_split = tf.split(inputs, channel_target, axis = 3)\n",
    "    output = tf.concat([phaseShift(x, scale, shape_1, shape_2) for x in input_split], axis = 3)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B_residual_block(inputs, output_dim, k_size, is_training, scope = 'G_b_res_block'):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        w1 = tf.get_variable('w1', [k_size, k_size, I.get_shape()[-1], output_dim],\n",
    "                            initializer = tf.truncated_normal_initializer(stddev = 0.2))\n",
    "        conv1 = tf.nn.conv2d(inputs, w1, strides = [1,1,1,1], padding = 'same')\n",
    "        b1 = tf.get_variable('b1', [output_dim], initializer = tf.constant_initializer(0.0))\n",
    "        conv1 = tf.nn.bias_add(conv1, b1)\n",
    "        \n",
    "        bn = tf.contrib.layers.batch_norm(conv1, is_training = is_training, scope = 'bn', \n",
    "                                           decay = 0.9, zero_debias_moving_mean = True)\n",
    "        prelu = PReLU(bn1)\n",
    "        \n",
    "        w2 = tf.get_variable('w2', [k_size, k_size, output_dim, output_dim],\n",
    "                            initializer = tf.truncated_normal_initializer(stddev = 0.2))\n",
    "        conv2 = tf.nn.conv2d(prelu, w2, strides = [1,1,1,1], padding = 'same')\n",
    "        b2 = tf.get_variable('b2', [output_dim], initializer = tf.constant_initializer(0.0))\n",
    "        conv2 = tf.nn.bias_add(conv2, b2)\n",
    "        \n",
    "        return conv2 + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_block(inputs, output_dim, k_size, scope = 'G_last_block'):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        w = tf.get_variable('w', [k_size, k_size, inputs.get_shape()[-1], output_dim],\n",
    "                           initializer = tf.truncated_normal_initializer(stddev = 0.2))\n",
    "        conv = tf.nn.conv2d(inputs, w, strides = [1,1,1,1], padding = 'same')\n",
    "        b = tf.get_variable('b', [output_dim], initializer = tf.constant_initializer(0.0))\n",
    "        conv = tf.nn.bias_add(conv, b)\n",
    "        px = pixelShuffler(conv)\n",
    "        return PReLU(px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(inputs, output_dims, k_size, s, is_training, scope = 'D_disc_block'):\n",
    "    with tf.variable_scope(scope) as scope:\n",
    "        w = tf.get_variable('w', [k_size, k_size, inputs.get_shape()[-1], output_dim],\n",
    "                           initializer = tf.truncated_normal_initializer(stddev = 0.2))\n",
    "        conv = tf.nn.conv2d(inputs, w, strides = [s,s,s,s], padding = 'same')\n",
    "        b = tf.get_variable('b', [output_dim], initializer = tf.constant_initializer(0.0))\n",
    "        conv = tf.nn.bias_add(conv, b)\n",
    "        return tf.nn.leaky_relu(conv, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "<img src=\"total_loss.PNG\"></img>\n",
    "<br></br>\n",
    "## Content loss\n",
    "* MSE based loss\n",
    "<img src=\"mse_loss.PNG\"></img>\n",
    "    - 가장 널리 쓰이지만 smooth한 영역에 대해서는 성능이 좋지 않다.\n",
    "    \n",
    "* VGG based loss\n",
    "<img src=\"vgg_loss.PNG\"></img>\n",
    "    - smooth한 영역에 대해서 성능을 개선시킬 수 있는 loss function\n",
    "    \n",
    "## Adversirial loss\n",
    "<img src=\"advers_loss.PNG\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRGAN:\n",
    "    def __init__(self, sess, checkpoint_dir, log_dir, img_dir, low_resolution_size, r, channel, \n",
    "                 feature_root = 64, batch_size = 1, lr = 0.0002, beta1 = 0.5, beta2 = 0.999,\n",
    "                 dropout = 0.5, loss_type = 'vgg'):\n",
    "        self.sess = sess\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.img_dir = img_dir\n",
    "        self.low_resolution_size = low_resolution_size\n",
    "        self.real_image_size = low_resolution_size * r\n",
    "        self.r = r\n",
    "        self.channel = channel\n",
    "        self.feature_root = 64\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.dropout = dropout\n",
    "        self.pool_k_size = 2\n",
    "        self.pool_s_size = 2\n",
    "        self.loss_type = loss_type\n",
    "        # saver 저장\n",
    "        \n",
    "    def generator(self, images):\n",
    "        # conv -> PReLU\n",
    "        with tf.variable_scope('G_start') as scope:\n",
    "            w = tf.get_variable('w', [9, 9, self.channel, self.feature_root],\n",
    "                               initializer = tf.truncated_normal_initializer(stddev = 0.2))\n",
    "            conv = tf.nn.conv2d(images, w, strides = [1,1,1,1], padding = 'same')\n",
    "            b = tf.get_variable('b', [self.feature_root], \n",
    "                               initializer = tf.constant_initializer(0.0))\n",
    "            conv = tf.nn.bias_add(conv, a)\n",
    "            prelu = PReLU(conv)\n",
    "        \n",
    "        skip_late = prelu\n",
    "        skip = prelu\n",
    "        \n",
    "        # 5 B residual blocks\n",
    "        for i in range(5):\n",
    "            skip = B_residual_block(skip, self.feature_root, 3, is_training = True, \n",
    "                                    scope = 'G_b_res_block' + str(i))\n",
    "            \n",
    "        with tf.variable_scope('G_middle') as scope:\n",
    "            w = tf.get_variable('w', [3, 3, self.feature_root, self.feature_root],\n",
    "                               initializer = tf.truncated_normal_initializer(stddev = 0.2))\n",
    "            conv = tf.nn.conv2d(skip, w, strides = [1,1,1,1], padding = 'same')\n",
    "            b = tf.get_variable('b', [self.feature_root], \n",
    "                               initializer = tf.constant_initializer(0.0))\n",
    "            conv = tf.nn.bias_add(conv, a)\n",
    "            bn = tf.contrib.layers.batch_norm(conv, is_training = is_training, scope = 'bn', \n",
    "                                           decay = 0.9, zero_debias_moving_mean = True)\n",
    "            es = bn + skip_late\n",
    "        \n",
    "        last = last_block(es, self.feature_root*4, scope = 'G_last_block_0')\n",
    "        last = last_block(last, self.feature_root*4, scope = 'G_last_block_1')\n",
    "        \n",
    "        with tf.variable_scope('G_last') as scope:\n",
    "            w = tf.get_variable('w', [3, 3, self.feature_root, self.feature_root],\n",
    "                               initializer = tf.truncated_normal_initializer(stddev = 0.2))\n",
    "            conv = tf.nn.conv2d(last, w, strides = [1,1,1,1], padding = 'same')\n",
    "            b = tf.get_variable('b', [self.feature_root], \n",
    "                               initializer = tf.constant_initializer(0.0))\n",
    "            conv = tf.nn.bias_add(conv, a)\n",
    "        \n",
    "        return conv\n",
    "    \n",
    "    def discriminator(self, images):\n",
    "        # Conv -> LeakyReLU\n",
    "        with tf.variable_scope('D_start') as scope:\n",
    "            w = tf.get_variable('w', [3, 3, self.feature_root, self.feature_root],\n",
    "                               initializer = tf.truncated_normal_initializer(stddev = 0.2))\n",
    "            conv = tf.nn.conv2d(last, w, strides = [1,1,1,1], padding = 'same')\n",
    "            b = tf.get_variable('b', [self.feature_root], \n",
    "                               initializer = tf.constant_initializer(0.0))\n",
    "            conv = tf.nn.bias_add(conv, a)\n",
    "            output = tf.nn.leaky_relu(conv, alpha = 0.2)\n",
    "        \n",
    "        features = self.feature_root\n",
    "        pools = []\n",
    "        for i in range(7):\n",
    "            output = vgg_block(output, features, (i+1)%2+1, scope = 'D_disc_block' + str(i))\n",
    "            if(i%2 == 0):\n",
    "                pools.append(output)\n",
    "                output = tf.nn.max_pool(output, ksize = [1, self.pool_k_size, self.pool_k_size, 1],\n",
    "                        strides = [1, self.pool_s_size, self.pool_s_size, 1], padding = 'SAME')\n",
    "                features = features * 2\n",
    "                \n",
    "        with tf.variable_scope('D_dense') as scope: \n",
    "            flat = tf.contrib.layers.flatten(output)\n",
    "            dense = tf.layers.dense(flat, 1024, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            dense = tf.nn.lrelu(dense)\n",
    "            dense = tf.layers.dense(dense, 1, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "            dense = tf.nn.sigmoid(dense)\n",
    "            \n",
    "        return dense, pools\n",
    "    \n",
    "    def content_loss_mse(hr, g_lr):\n",
    "        hr_flat = tf.layers.flatten(hr)\n",
    "        g_lr_flat = tf.layers.flatten(g_lr)\n",
    "        return tf.reduce_mean((hr_flat - g_lr_flat)**2)\n",
    "    \n",
    "    def content_loss_vgg(real_pools, fake_pools):\n",
    "        total_flat = 0\n",
    "        for vm_real, vm_fake in zip(real_pools, fake_pools):\n",
    "            flat_real = tf.layers.flatten(vm_real)\n",
    "            flat_fake = tf.layers.flatten(vm_fake)\n",
    "            if total_flat == 0:\n",
    "                total_flat = (flat_real - flat_fake)**2\n",
    "            else:\n",
    "                total_flat = tf.concate([total_flat, (flat_real - flat_fake)**2], 1)\n",
    "        \n",
    "        return tf.reduce_mean(total_flat)\n",
    "    \n",
    "    # ONLY for generator\n",
    "    def adversarial_loss(fake_logits):\n",
    "        return tf.reduce_mean(-tf.log(fake_logits))\n",
    "            \n",
    "    def build_model(self):\n",
    "        real_images = tf.placeholder(tf.float32, name = 'real_images',\n",
    "                                    shape = [None, self.real_image_size[0], self.real_image_size[1], self.channel])\n",
    "        low_resolutions = tf.placeholder(tf.float32, name = 'low_resolutions',\n",
    "                                    shape = [None, self.low_resolution_size[0], self.low_resolution_size[1], self.channel])\n",
    "        \n",
    "        real_logits, real_pools = discriminator(real_images)\n",
    "        fake_images = generator(low_resolutions)\n",
    "        fake_logits, fake_pools = discriminator(fake_images)\n",
    "        \n",
    "        d_loss = tf.reduce_mean(tf.log(real_logits) + tf.log(1-fake_logits))\n",
    "        if self.loss_type == 'vgg':\n",
    "            g_loss = content_loss_vgg(real_pools, fake_pools) + adversarial_loss(fake_logits)\n",
    "        else:\n",
    "            g_loss = content_loss_mse(real_image, fake_images) + adversarial_loss(fake_logits)\n",
    "            \n",
    "        tvar = tf.trainable_variables()\n",
    "        dvar = [var for var in tvar if 'D' in var.name]\n",
    "        gvar = [var for var in tvar if 'G' in var.name] \n",
    "        \n",
    "        d_train_step = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = self.beta1, \n",
    "                                              beta2 = self.beta2).minimize(d_loss, var_list = dvar)\n",
    "        g_train_step = tf.train.AdamOptimizer(learning_rate = self.lr, beta1 = self.beta1,\n",
    "                                             beta2 = self.beta2).minimize(g_loss, var_list = gvar)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
